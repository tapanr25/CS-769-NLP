{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 54. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Map: 100%|██████████| 3440/3440 [00:00<00:00, 4811.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['seq', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 3440\n",
      "})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tapan\\Documents\\3Sem\\NLP769\\cs769-assignments-main\\assignment3\\evaluate.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39mprint\u001b[39m(AP)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     \u001b[39m# outputs = model(input_ids=tokenized_dataset['test']['input_ids'], attention_mask=tokenized_dataset['test']['attention_mask'])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     \u001b[39m# predicted_label = outputs.logits.argmax(-1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     \u001b[39m# print(predicted_label)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m evaluate_testdata(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvdjdb_internal_negatives_data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtmp\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mvocab1\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcheckpoint-25800\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\tapan\\Documents\\3Sem\\NLP769\\cs769-assignments-main\\assignment3\\evaluate.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m atm\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(tokenized_dataset[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     out\u001b[39m=\u001b[39mmodel(iid,atm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msigmoid(out\u001b[39m.\u001b[39mlogits)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tapan/Documents/3Sem/NLP769/cs769-assignments-main/assignment3/evaluate.ipynb#W0sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m predictions\u001b[39m=\u001b[39m(torch\u001b[39m.\u001b[39margmax(preds, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\tapan\\anaconda3\\envs\\839\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tapan\\anaconda3\\envs\\839\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:1120\u001b[0m, in \u001b[0;36mEsmForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1120\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mesm(\n\u001b[0;32m   1121\u001b[0m     input_ids,\n\u001b[0;32m   1122\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1123\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1124\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1125\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1126\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1127\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1128\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1129\u001b[0m )\n\u001b[0;32m   1130\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1131\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\tapan\\anaconda3\\envs\\839\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tapan\\anaconda3\\envs\\839\\Lib\\site-packages\\transformers\\models\\esm\\modeling_esm.py:886\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 886\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[0;32m    887\u001b[0m device \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    889\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, EsmTokenizer, pipeline\n",
    "import sys\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn.metrics\n",
    "\n",
    "def precision_recall_curve(y_true, pred_scores, thresholds):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = [\"TRUE\" if score >= threshold else \"FALSE\" for score in pred_scores]\n",
    "\n",
    "        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"TRUE\")\n",
    "        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"TRUE\")\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "def evaluate_testdata(testdata, model_path, sep):\n",
    "\n",
    "\n",
    "    model = EsmForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "    tokenizer.add_tokens([sep])\n",
    "    epitope_vocab = [\"A1\", \"C1\", \"D1\", \"E1\", \"F1\", \"G1\", \"H1\", \"I1\", \"K1\", \"L1\", \"M1\", \"N1\", \"P1\", \"Q1\", \"R1\", \"S1\", \"T1\", \"V1\", \"W1\", \"Y1\"]\n",
    "\n",
    "    ###########  comment for 1 vocab\n",
    "    tokenizer.add_tokens(epitope_vocab)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    test_df = pd.read_csv(testdata)\n",
    "    test_df['label_true_pair']=test_df['label_true_pair'].astype('int')\n",
    "\n",
    "    def insert_1_after_characters(s):\n",
    "        return '1'.join(s) + '1'\n",
    "\n",
    "    ### comment for 1 vocab\n",
    "    test_df['epitope_aa'] = test_df['epitope_aa'].apply(insert_1_after_characters)\n",
    "\n",
    "    # Format data\n",
    "    test_df = pd.DataFrame({'seq': test_df['cdr3_alpha_aa'] + sep + test_df['epitope_aa'],\n",
    "                            'label': test_df['label_true_pair']})\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'test': Dataset.from_pandas(test_df)\n",
    "    })\n",
    "\n",
    "    def tokenize_function(dataset):\n",
    "        return tokenizer(dataset['seq'], return_tensors='pt', max_length=len(tokenizer), padding='max_length', truncation=True)\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
    "    print(tokenized_dataset['test'])\n",
    "\n",
    "    # saved_model = pipeline('text-classification',\n",
    "    #                    model = model_path, tokenizer=tokenizer)\n",
    "    # predictions = saved_model(tokenized_dataset['test'], truncation=True)\n",
    "    # print(predictions)\n",
    "    iid=torch.tensor(tokenized_dataset['test']['input_ids']).to('cuda')\n",
    "    atm=torch.tensor(tokenized_dataset['test']['attention_mask']).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        out=model(iid,atm)\n",
    "    preds = torch.nn.functional.sigmoid(out.logits)\n",
    "    # predictions=(torch.argmax(preds, dim=1))\n",
    "    thresholds=np.arange(start=0.2, stop=0.7, step=0.05)\n",
    "\n",
    "    y_true = test_df['label_true_pair']\n",
    "    # pred_scores = predictions['scores']\n",
    "    precisions, recalls = precision_recall_curve(y_true=y_true,\n",
    "                                                pred_scores=preds[:,0],\n",
    "                                                thresholds=thresholds)\n",
    "\n",
    "    precisions.append(1)\n",
    "    recalls.append(0)\n",
    "\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "\n",
    "    AP = np.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\n",
    "    print(AP)\n",
    "\n",
    "\n",
    "\n",
    "    # outputs = model(input_ids=tokenized_dataset['test']['input_ids'], attention_mask=tokenized_dataset['test']['attention_mask'])\n",
    "    # predicted_label = outputs.logits.argmax(-1)\n",
    "    # print(predicted_label)\n",
    "\n",
    "\n",
    "evaluate_testdata(r'vdjdb_internal_negatives_data.csv',r'tmp\\vocab1\\checkpoint-25800', '0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "839",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

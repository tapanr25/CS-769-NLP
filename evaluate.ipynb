{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, EsmTokenizer, pipeline\n",
    "import sys\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn.metrics\n",
    "\n",
    "def precision_recall_curve(y_true, pred_scores, thresholds):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = [\"TRUE\" if score >= threshold else \"FALSE\" for score in pred_scores]\n",
    "\n",
    "        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, pos_label=\"TRUE\")\n",
    "        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, pos_label=\"TRUE\")\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "def evaluate_testdata(testdata, model_path, sep):\n",
    "\n",
    "\n",
    "    model = EsmForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "    tokenizer.add_tokens([sep])\n",
    "    epitope_vocab = [\"A1\", \"C1\", \"D1\", \"E1\", \"F1\", \"G1\", \"H1\", \"I1\", \"K1\", \"L1\", \"M1\", \"N1\", \"P1\", \"Q1\", \"R1\", \"S1\", \"T1\", \"V1\", \"W1\", \"Y1\"]\n",
    "\n",
    "    ###########  comment for 1 vocab\n",
    "    tokenizer.add_tokens(epitope_vocab)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    test_df = pd.read_csv(testdata)\n",
    "    test_df['label_true_pair']=test_df['label_true_pair'].astype('int')\n",
    "\n",
    "    def insert_1_after_characters(s):\n",
    "        return '1'.join(s) + '1'\n",
    "\n",
    "    ### comment for 1 vocab\n",
    "    test_df['epitope_aa'] = test_df['epitope_aa'].apply(insert_1_after_characters)\n",
    "\n",
    "    # Format data\n",
    "    test_df = pd.DataFrame({'seq': test_df['cdr3_alpha_aa'] + sep + test_df['epitope_aa'],\n",
    "                            'label': test_df['label_true_pair']})\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'test': Dataset.from_pandas(test_df)\n",
    "    })\n",
    "\n",
    "    def tokenize_function(dataset):\n",
    "        return tokenizer(dataset['seq'], return_tensors='pt', max_length=len(tokenizer), padding='max_length', truncation=True)\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
    "    print(tokenized_dataset['test'])\n",
    "\n",
    "    # saved_model = pipeline('text-classification',\n",
    "    #                    model = model_path, tokenizer=tokenizer)\n",
    "    # predictions = saved_model(tokenized_dataset['test'], truncation=True)\n",
    "    # print(predictions)\n",
    "    iid=torch.tensor(tokenized_dataset['test']['input_ids']).to('cuda')\n",
    "    atm=torch.tensor(tokenized_dataset['test']['attention_mask']).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        out=model(iid,atm)\n",
    "    preds = torch.nn.functional.sigmoid(out.logits)\n",
    "    # predictions=(torch.argmax(preds, dim=1))\n",
    "    thresholds=np.arange(start=0.2, stop=0.7, step=0.05)\n",
    "\n",
    "    y_true = test_df['label_true_pair']\n",
    "    # pred_scores = predictions['scores']\n",
    "    precisions, recalls = precision_recall_curve(y_true=y_true,\n",
    "                                                pred_scores=preds[:,0],\n",
    "                                                thresholds=thresholds)\n",
    "\n",
    "    precisions.append(1)\n",
    "    recalls.append(0)\n",
    "\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "\n",
    "    AP = np.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\n",
    "    print(AP)\n",
    "\n",
    "\n",
    "\n",
    "    # outputs = model(input_ids=tokenized_dataset['test']['input_ids'], attention_mask=tokenized_dataset['test']['attention_mask'])\n",
    "    # predicted_label = outputs.logits.argmax(-1)\n",
    "    # print(predicted_label)\n",
    "\n",
    "\n",
    "evaluate_testdata(r'vdjdb_external_negatives_data.csv',r'tmp\\vocab1\\checkpoint-25800', '0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source of Borrowed Code:**\n",
    "\n",
    "The following code was adapted from (https://blog.paperspace.com/mean-average-precision/) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "839",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

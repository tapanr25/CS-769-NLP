{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, logging, \\\n",
    "    Trainer, TrainingArguments\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(y_true, pred_scores, thresholds):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = [1 if score >= threshold else 0 for score in pred_scores]\n",
    "        # print(y_pred)\n",
    "        precision = sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred)\n",
    "        recall = sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of cuda devices: 1\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2406/2406 [00:00<00:00, 5882.66 examples/s]\n",
      "Map: 100%|██████████| 3438/3438 [00:00<00:00, 6390.25 examples/s]\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 34. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading trainer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91      2865\n",
      "           1       0.66      0.13      0.22       573\n",
      "\n",
      "    accuracy                           0.84      3438\n",
      "   macro avg       0.75      0.56      0.57      3438\n",
      "weighted avg       0.82      0.84      0.80      3438\n",
      "\n",
      "Avg mean precision:  0.14135146273177523\n"
     ]
    }
   ],
   "source": [
    "def configure_tester(tokenizer, tokenized_dataset, model_name, sep, checkpoint=r'tmp\\stapler_esm2_t6_8M_UR50D_0_epv1_aabb\\checkpoint-18834'):    \n",
    "    model = EsmForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Initialize wandb\n",
    "    # wandb.init(project='stapler_esm', name=f'{model_name}_{sep}_ep_pc_v1')\n",
    "\n",
    "    # Configure training arguments\n",
    "    training_args = TrainingArguments(output_dir=f'tmp/stapler_{model_name}_{sep}_epv1',\n",
    "                                      evaluation_strategy='epoch',\n",
    "                                      per_device_train_batch_size=64,\n",
    "                                      per_device_eval_batch_size=64,\n",
    "                                      num_train_epochs=100,\n",
    "                                      logging_strategy='epoch',\n",
    "                                      learning_rate=0.000001,\n",
    "                                      save_total_limit=1,\n",
    "                                      \n",
    "                                      \n",
    "                                      load_best_model_at_end=True,\n",
    "                                      metric_for_best_model=\"accuracy\",\n",
    "                                      save_strategy='epoch',)\n",
    "\n",
    "    # Configure metrics\n",
    "    metric = evaluate.load('accuracy')\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Instantiate Trainer\n",
    "    trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      train_dataset=tokenized_dataset['train'],\n",
    "                      eval_dataset=tokenized_dataset['test'],\n",
    "                      compute_metrics=compute_metrics)\n",
    "    return trainer, model\n",
    "\n",
    "\n",
    "# def main(model_name, sep):\n",
    "#     \"\"\"\n",
    "#     Entry point of the program.\n",
    "\n",
    "#     \"\"\"\n",
    "model_name, sep='esm2_t6_8M_UR50D', '0'\n",
    "# Load data\n",
    "print('No. of cuda devices:', torch.cuda.device_count())\n",
    "df = pd.read_csv('vdjdb_external_negatives_data.csv')\n",
    "# test_df = pd.read_csv('train-set_full-seq.csv')[0.7*23544:]\n",
    "df['label_true_pair']=df['label_true_pair'].astype('int')\n",
    "\n",
    "def insert_1_after_characters(s):\n",
    "    return '1'.join(s) + '1'\n",
    "# train_df['seq_2'] = train_df['seq_2'].apply(insert_1_after_characters)\n",
    "# test_df['seq_2'] = test_df['seq_2'].apply(insert_1_after_characters)\n",
    "\n",
    "### comment for 1 vocab\n",
    "# df['epitope_aa'] = df['epitope_aa'].apply(insert_1_after_characters)\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.3)\n",
    "# print((train_df).head())\n",
    "# Format data\n",
    "test_df=df\n",
    "train_df = pd.DataFrame({'seq': train_df['cdr3_alpha_aa'] + sep + train_df['epitope_aa']+ sep +train_df['cdr3_beta_aa'],\n",
    "                            'label': train_df['label_true_pair']})\n",
    "test_df = pd.DataFrame({'seq': test_df['cdr3_alpha_aa'] + sep + test_df['epitope_aa']+ sep +test_df['cdr3_beta_aa'],\n",
    "                        'label': test_df['label_true_pair']})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})\n",
    "print('loading model')\n",
    "# Load tokenizer and add custom tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'facebook/{model_name}')\n",
    "tokenizer.add_tokens([sep])\n",
    "epitope_vocab = [\"A1\", \"C1\", \"D1\", \"E1\", \"F1\", \"G1\", \"H1\", \"I1\", \"K1\", \"L1\", \"M1\", \"N1\", \"P1\", \"Q1\", \"R1\", \"S1\", \"T1\", \"V1\", \"W1\", \"Y1\"]\n",
    "\n",
    "###########  comment for 1 vocab\n",
    "# tokenizer.add_tokens(epitope_vocab)\n",
    "\n",
    "# Tokenize sequences\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset['seq'], return_tensors='pt', max_length=len(tokenizer), padding='max_length', truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, batch_size=16)\n",
    "\n",
    "\n",
    "print('loading trainer')\n",
    "# Configure Trainer\n",
    "trainer, model = configure_tester(tokenizer, tokenized_dataset, model_name, sep)\n",
    "iid=torch.tensor(tokenized_dataset['test']['input_ids']).to('cuda')\n",
    "atm=torch.tensor(tokenized_dataset['test']['attention_mask']).to('cuda')\n",
    "with torch.no_grad():\n",
    "    out=model(iid,atm)\n",
    "\n",
    "preds = torch.nn.functional.sigmoid(out.logits)\n",
    "predictions=(torch.argmax(preds, dim=1))\n",
    "print(classification_report(y_pred=predictions.tolist(), y_true=(tokenized_dataset['test']['label'])))\n",
    "\n",
    "thresholds=np.arange(start=0.2, stop=0.7, step=0.05)\n",
    "\n",
    "\n",
    "# pred_scores = predictions['scores']\n",
    "precisions, recalls = precision_recall_curve(y_true=(tokenized_dataset['test']['label']),\n",
    "                                            pred_scores=preds[:,0].tolist(),\n",
    "                                            thresholds=thresholds)\n",
    "precisions.append(1)\n",
    "recalls.append(0)\n",
    "\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "AP = np.sum((recalls[:-1] - recalls[1:]) * precisions[:-1])\n",
    "print('Avg mean precision: ', AP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "839",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
